{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aUZ17c7N4ol-",
    "outputId": "2266d1bd-8bb3-456e-cb6d-7acaf962dcb1"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#!pip install sweetviz\n",
    "#sweet_report = sv.analyze(df)\n",
    "#sweet_report.show_html('sweet_report.html')\n",
    "!pip install pyclustering\n",
    "!pip install tqdm\n",
    "!pip install python-dotenv\n",
    "!pip install seaborn\n",
    "!pip install beautifulsoup4\n",
    "!pip install country-named-entity-recognition\n",
    "!pip install flickrapi\n",
    "!pip install geopy\n",
    "!pip install datasketch\n",
    "!pip install lightfm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oacFaXH9Opp",
    "outputId": "28486d7a-431d-4867-f098-1fc4858ecb81"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "#import networkx as nx\n",
    "#from networkx.algorithms import community\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pyclustering.cluster import cluster_visualizer\n",
    "from pyclustering.cluster.cure import cure\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import psutil\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "\n",
    "\n",
    "from multiprocessing import process\n",
    "from re import M\n",
    "from typing import Counter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from country_named_entity_recognition import find_countries\n",
    "import flickrapi\n",
    "from geopy.geocoders import Nominatim\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "import math\n",
    "import lightfm\n",
    "from lightfm import LightFM\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.datasets import make_blobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##START OF FLICKR PART\n",
    "def configure():\n",
    "    load_dotenv()\n",
    "\n",
    "configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NomadList data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filepath):\n",
    "    current_path = os.getcwd()\n",
    "    df = pd.read_csv(current_path+filepath)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_nomadlist = '/datasets/nomadlist/cities_predict.csv'\n",
    "df_nomadlist = get_data(filepath_nomadlist)\n",
    "\n",
    "##IMPORTING SDG DATA\n",
    "filepath_SDG = \"/datasets/SDG_CSV/SDGData.csv\"\n",
    "df_SDG = get_data(filepath_SDG)\n",
    "df_SDG = df_SDG[['Country Code', 'Country Name', 'Indicator Name', '2015']]\n",
    "df_SDG = df_SDG.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flickr data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_name_from_nomadlist(df_nomadlist):\n",
    "    cities = df_nomadlist['place_slug']\n",
    "    countries = df_nomadlist['country']\n",
    "    cities_list = []\n",
    "    for city, country in zip(cities, countries):\n",
    "        number_of_country_words = len(country.split(' '))\n",
    "        city_name = ' '.join(city.split('-')[:-number_of_country_words])\n",
    "        cities_list.append(city_name)\n",
    "\n",
    "    cities_list = sorted(cities_list)\n",
    "    return cities_list\n",
    "\n",
    "def get_geoloc_per_city(cities_list, saving_filepath):\n",
    "    current_path = os.getcwd()\n",
    "    geolocator = Nominatim(user_agent='myapplication')\n",
    "    cities_coord = {}\n",
    "\n",
    "    if os.path.exists(current_path+saving_filepath) and os.path.getsize(current_path+saving_filepath) > 0:\n",
    "        f = open(current_path+saving_filepath)\n",
    "        cities_coord = json.load(f)\n",
    "    else:\n",
    "        for city in tqdm(cities_list):\n",
    "            location = geolocator.geocode(city)\n",
    "            lat = location.raw['lat']\n",
    "            lon = location.raw['lon']\n",
    "            cities_coord[city] = (lat, lon)\n",
    "\n",
    "        with open('cities_coord.json', 'w') as fp:\n",
    "            json.dump(cities_coord, fp)\n",
    "    return cities_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flickr_data(cities_coord, saving_filepath):\n",
    "    current_path = os.getcwd()\n",
    "    if os.path.exists(current_path+saving_filepath) and os.path.getsize(current_path+saving_filepath) > 0:\n",
    "        f = open(current_path+saving_filepath)\n",
    "        users = json.load(f)\n",
    "    else:\n",
    "        flickr = flickrapi.FlickrAPI(os.getenv('api_key'), os.getenv('api_secret'), format='parsed-json')\n",
    "        extras = ['description','tags','url_sq', 'url_t', 'url_s', 'url_q', 'url_m', 'url_n', 'url_z', 'url_c', 'url_l', 'url_o']\n",
    "        users = {}\n",
    "        for city, coords in tqdm(cities_coord.items()): \n",
    "            try:\n",
    "                images = flickr.photos.search(text='travel', lat = coords[0], lon = coords[1], radius = '30', radius_units = 'km', extras=extras)\n",
    "                for image in images['photos']['photo']:\n",
    "                    user = image['owner']\n",
    "                    if user in users.keys():\n",
    "                        users[user].append(city)\n",
    "                    else:\n",
    "                        users[user] = [city]\n",
    "            except:\n",
    "                print(f'Images for {city} not found or another error encountered')\n",
    "\n",
    "            with open(saving_filepath, 'w') as fp:\n",
    "                json.dump(users, fp)\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_list = get_city_name_from_nomadlist(df_nomadlist)\n",
    "\n",
    "#Flickr part\n",
    "saving_filepath = '/datasets/flickr/cities_coord.json'\n",
    "cities_coord = get_geoloc_per_city(cities_list, saving_filepath)\n",
    "saving_filepath = '/datasets/flickr/users.json'\n",
    "users_raw = get_flickr_data(cities_coord, saving_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_by_country(df_nomadlist, df_SDG):\n",
    "    filtered_country_df_nomadlist = df_nomadlist['country'].unique()\n",
    "    country_codes = {find_countries(country)[0][0].alpha_3: country for country in filtered_country_df_nomadlist if find_countries(country)}\n",
    "\n",
    "    #Skim the SDG dataset\n",
    "    for code in country_codes.keys():\n",
    "        df_SDG.loc[df_SDG['Country Code'] == code, 'Country Name'] = country_codes[code]\n",
    "\n",
    "    df_SDG = df_SDG.loc[df_SDG['Country Name'].isin(filtered_country_df_nomadlist)]\n",
    "    number_of_countries = len(df_SDG['Country Code'].unique())\n",
    "\n",
    "    #Finding columns that preserve all data, i.e. that are shared by every city\n",
    "    df_SDG_count = df_SDG.groupby(['Indicator Name']).count()\n",
    "    common_indicators = df_SDG_count[df_SDG_count['2015'] == number_of_countries].reset_index()['Indicator Name']\n",
    "    df_SDG = df_SDG.loc[df_SDG['Indicator Name'].isin(common_indicators)]\n",
    "\n",
    "\n",
    "    #Actually merge the data\n",
    "    df_SDG = df_SDG[['Country Name', 'Indicator Name', '2015']]\n",
    "    df_SDG = df_SDG.pivot(index = 'Country Name', columns = 'Indicator Name', values = '2015')\n",
    "\n",
    "    df_SDG = df_SDG.reset_index()\n",
    "    df_SDG_cleaned = df_SDG.rename(columns={'Country Name': 'country'})\n",
    "\n",
    "    df_nomadlist = df_nomadlist.merge(df_SDG_cleaned, how='outer', on='country')\n",
    "    return df_nomadlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nomadlist = merge_data_by_country(df_nomadlist, df_SDG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(raw_users):\n",
    "    users_cities_rating = {}\n",
    "    number_of_values = 0\n",
    "    users = {}\n",
    "    for key, value in raw_users.items():\n",
    "        uniques = set(value)\n",
    "        avg_pics_per_place = len(value)/len(uniques)\n",
    "        city_count = Counter(value)\n",
    "        maximum_city = max(city_count, key=city_count.get)\n",
    "        maximum_pics_per_user = city_count[maximum_city]\n",
    "        #Normalization\n",
    "        #We want a rating from 0 to 5\n",
    "        for item, count in city_count.items():\n",
    "            city_count[item] = math.ceil(city_count[item]/(maximum_pics_per_user/5))\n",
    "            #city_count[item] /= avg_pics_per_place\n",
    "\n",
    "        users_cities_rating[key] = city_count\n",
    "\n",
    "        users[key] = uniques\n",
    "        number_of_values += len(users[key])\n",
    "\n",
    "    return users, users_cities_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, users_cities_rating = process_data(users_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1br_studio_rent_in_center</th>\n",
       "      <th>adult_nightlife</th>\n",
       "      <th>air_quality_(year-round)</th>\n",
       "      <th>airbnb_(monthly)</th>\n",
       "      <th>cashless_society</th>\n",
       "      <th>coca-cola</th>\n",
       "      <th>coffee</th>\n",
       "      <th>cost_of_living</th>\n",
       "      <th>cost_of_living_for_expat</th>\n",
       "      <th>cost_of_living_for_local</th>\n",
       "      <th>...</th>\n",
       "      <th>startup_score</th>\n",
       "      <th>traffic_safety</th>\n",
       "      <th>walkability</th>\n",
       "      <th>nomad_score</th>\n",
       "      <th>region</th>\n",
       "      <th>country</th>\n",
       "      <th>place_slug</th>\n",
       "      <th>Access to electricity (% of population)</th>\n",
       "      <th>Access to electricity, urban (% of urban population)</th>\n",
       "      <th>Renewable electricity output (% of total electricity output)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>492.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>946.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.0</td>\n",
       "      <td>961.0</td>\n",
       "      <td>626.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.03</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>mexico-city-mexico</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.300003</td>\n",
       "      <td>15.394134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>223.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>976.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.51</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>cancun-mexico</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.300003</td>\n",
       "      <td>15.394134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>367.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2074.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.54</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>tulum-mexico</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.300003</td>\n",
       "      <td>15.394134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>369.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1617.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>938.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.17</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>playa-del-carmen-mexico</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.300003</td>\n",
       "      <td>15.394134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.54</td>\n",
       "      <td>3.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.20</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>guadalajara-mexico</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.300003</td>\n",
       "      <td>15.394134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1br_studio_rent_in_center  adult_nightlife  air_quality_(year-round)  \\\n",
       "0                      492.0              4.0                      42.0   \n",
       "1                      223.0              3.0                      19.0   \n",
       "2                      367.0              2.0                      54.0   \n",
       "3                      369.0              2.0                      53.0   \n",
       "4                      216.0              4.0                      41.0   \n",
       "\n",
       "   airbnb_(monthly)  cashless_society  coca-cola  coffee  cost_of_living  \\\n",
       "0             946.0               1.0       0.70    1.40             3.0   \n",
       "1             976.0               1.0       0.63    1.11             3.0   \n",
       "2            2074.0               1.0       0.66    1.94             3.0   \n",
       "3            1617.0               1.0       0.61    1.23             3.0   \n",
       "4             641.0               1.0       0.67    1.54             3.0   \n",
       "\n",
       "   cost_of_living_for_expat  cost_of_living_for_local  ...  startup_score  \\\n",
       "0                     961.0                     626.0  ...            3.0   \n",
       "1                     697.0                     349.0  ...            3.0   \n",
       "2                    1072.0                     512.0  ...            3.0   \n",
       "3                     938.0                     483.0  ...            3.0   \n",
       "4                     627.0                     380.0  ...            3.0   \n",
       "\n",
       "   traffic_safety  walkability  nomad_score         region  country  \\\n",
       "0             4.0          4.0         4.03  Latin America   Mexico   \n",
       "1             4.0          4.0         4.51  Latin America   Mexico   \n",
       "2             4.0          1.0         3.54  Latin America   Mexico   \n",
       "3             4.0          4.0         4.17  Latin America   Mexico   \n",
       "4             4.0          4.0         4.20  Latin America   Mexico   \n",
       "\n",
       "                place_slug  Access to electricity (% of population)  \\\n",
       "0       mexico-city-mexico                                     99.0   \n",
       "1            cancun-mexico                                     99.0   \n",
       "2             tulum-mexico                                     99.0   \n",
       "3  playa-del-carmen-mexico                                     99.0   \n",
       "4       guadalajara-mexico                                     99.0   \n",
       "\n",
       "   Access to electricity, urban (% of urban population)  \\\n",
       "0                                          99.300003      \n",
       "1                                          99.300003      \n",
       "2                                          99.300003      \n",
       "3                                          99.300003      \n",
       "4                                          99.300003      \n",
       "\n",
       "   Renewable electricity output (% of total electricity output)  \n",
       "0                                          15.394134             \n",
       "1                                          15.394134             \n",
       "2                                          15.394134             \n",
       "3                                          15.394134             \n",
       "4                                          15.394134             \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_nomadlist\n",
    "df.isnull().sum()\n",
    "\n",
    "df.duplicated().sum()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Plot the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(25, 25))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot3dClusterRepresentation(df, num_cols = 4, hasClusterColumn = True):\n",
    "    # Create a grid of subplots with 4 plots per line\n",
    "    num_rows = int(np.ceil(len(range(0, 360, 30)) / num_cols))\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 3 * num_rows), subplot_kw={'projection': '3d'})\n",
    "\n",
    "    # Iterate over different angles\n",
    "    for i, angle in enumerate(range(0, 360, 30)):\n",
    "        row_index = i // num_cols\n",
    "        col_index = i % num_cols\n",
    "\n",
    "        if hasClusterColumn:\n",
    "        # Scatter plot points by cluster\n",
    "            for cluster in range(num_clusters):\n",
    "                  cluster_data = df[df['Cluster'] == cluster]\n",
    "                  axes[row_index, col_index].scatter(cluster_data['PCA1'], cluster_data['PCA2'], cluster_data['PCA3'], label=f'Cluster {cluster + 1}')\n",
    "        else:\n",
    "            axes[row_index, col_index].scatter(df['PCA1'], df['PCA2'], df['PCA3'])\n",
    "\n",
    "        axes[row_index, col_index].set_xlabel('PCA1')\n",
    "        axes[row_index, col_index].set_ylabel('PCA2')\n",
    "        axes[row_index, col_index].set_zlabel('PCA3')\n",
    "        axes[row_index, col_index].set_title(f'3D Scatter Plot of Clusters (Angle: {angle}°')\n",
    "\n",
    "        # Set the viewing angle\n",
    "        axes[row_index, col_index].view_init(30, angle)\n",
    "\n",
    "        # Show the legend in the first plot of each row\n",
    "        if col_index == 0:\n",
    "            axes[row_index, col_index].legend()\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove cost of living both columns\n",
    "columns_to_drop = ['coca-cola','adult_nightlife','coffee','region', 'place_slug','country','cost_of_living_for_expat','1br_studio_rent_in_center','airbnb_(monthly)']\n",
    "df_for_clustering = df.drop(columns=columns_to_drop)\n",
    "df_for_clustering['cost_of_living_for_local'] = - df_for_clustering['cost_of_living_for_local']\n",
    "df_for_clustering.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((1,5))\n",
    "normalized_features = scaler.fit_transform(df_for_clustering)\n",
    "\n",
    "# Round the normalized values to whole numbers\n",
    "rounded_features = pd.DataFrame(normalized_features).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_clustering23D = rounded_features.copy()\n",
    "\n",
    "# Apply k-means clustering\n",
    "num_clusters = 4 # You can choose the number of clusters based on your needs\n",
    "kmeans_23D = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df_for_clustering23D['Cluster'] = kmeans_23D.fit_predict(df_for_clustering23D.values)\n",
    "\n",
    "# Apply PCA to reduce dimensionality to 3D\n",
    "pca = PCA(n_components=3)\n",
    "df_for_clustering23D_pca = pca.fit_transform(df_for_clustering23D.values)\n",
    "\n",
    "# Create a DataFrame with the PCA components and the cluster labels\n",
    "df_pca_cluster = pd.DataFrame(data=df_for_clustering23D_pca, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "df_pca_cluster['Cluster'] = df_for_clustering23D['Cluster']\n",
    "\n",
    "plot3dClusterRepresentation(df_pca_cluster,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features from the DataFrame\n",
    "X = rounded_features.values\n",
    "\n",
    "# Apply PCA with 3 components\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Create a new DataFrame with the PCA results\n",
    "df_pca = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "\n",
    "plot3dClusterRepresentation(df_pca,4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "inertias = []\n",
    "mapping1 = {}\n",
    "mapping2 = {}\n",
    "K = range(1, 12)\n",
    "\n",
    "for k in K:\n",
    "    # Building and fitting the model\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(X_pca)\n",
    "    kmeanModel.fit(X_pca)\n",
    "\n",
    "    distortions.append(sum(np.min(cdist(X_pca, kmeanModel.cluster_centers_,\n",
    "                                        'euclidean'), axis=1)) / X_pca.shape[0])\n",
    "    inertias.append(kmeanModel.inertia_)\n",
    "\n",
    "    mapping1[k] = sum(np.min(cdist(X_pca, kmeanModel.cluster_centers_,\n",
    "                                   'euclidean'), axis=1)) / X_pca.shape[0]\n",
    "    mapping2[k] = kmeanModel.inertia_\n",
    "\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method using Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply k-means clustering\n",
    "num_clusters = 5  # You can choose the number of clusters based on your needs\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df_pca['Cluster'] = kmeans.fit_predict(df_pca[['PCA1', 'PCA2', 'PCA3']])\n",
    "\n",
    "plot3dClusterRepresentation(df_pca,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster assignments back to the original DataFrame\n",
    "df['Cluster'] = df_pca['Cluster']\n",
    "\n",
    "num_clusters = 6\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_cities = df[df['Cluster'] == cluster].nlargest(5, 'nomad_score')\n",
    "    plt.scatter(\n",
    "        cluster_cities['place_slug'],\n",
    "        cluster_cities['nomad_score'],\n",
    "        label=f'Cluster {cluster + 1}',\n",
    "        alpha=0.7,\n",
    "        s=cluster_cities['nomad_score'] * 10  # Adjust the multiplier based on the desired marker size\n",
    "    )\n",
    "\n",
    "# Set labels and title\n",
    "plt.title('Top 5 Cities with Color-Coded Clusters and Marker Size Based on Nomad Score')\n",
    "plt.xlabel('place_slug')\n",
    "plt.ylabel('nomad_score')\n",
    "\n",
    "# Display legend\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix temp['place_slug'] format\n",
    "temp = df[['place_slug','country','Cluster']].copy()\n",
    "\n",
    "for i in range(len(temp['place_slug'])):\n",
    "    number_of_country_words = len(temp['country'][i].split(' '))\n",
    "    temp.loc[i, 'place_slug'] = ' '.join(temp['place_slug'][i].split('-')[:-number_of_country_words])\n",
    "\n",
    "# Load JSON data\n",
    "with open('cities_coord.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create a DataFrame from the JSON data\n",
    "df_cities_coord = pd.DataFrame.from_dict(data, orient='index', columns=['Latitude', 'Longitude']).reset_index()\n",
    "df_cities_coord = df_cities_coord.rename(columns={'index': 'place_slug'})\n",
    "\n",
    "df_map_plot = pd.merge(df_cities_coord, temp, how='inner', on='place_slug')\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "geometry = [Point(xy) for xy in zip(df_map_plot['Longitude'].astype(float), df_map_plot['Latitude'].astype(float))]\n",
    "gdf = GeoDataFrame(df_map_plot, geometry=geometry)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world.plot(ax=ax, color='lightgray')\n",
    "\n",
    "# Create a color map for clusters\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown']\n",
    "cluster_colors = dict(zip(gdf['Cluster'].unique(), colors))\n",
    "\n",
    "\n",
    "# Plot each cluster with a different color\n",
    "for cluster, color in cluster_colors.items():\n",
    "    subset = gdf[gdf['Cluster'] == cluster]\n",
    "    subset.plot(ax=ax, marker='o', color=color, markersize=10, label=f'Cluster {cluster}', alpha=0.7)\n",
    "\n",
    "# Add legend with ordered labels\n",
    "ordered_labels = sorted(gdf['Cluster'].unique())\n",
    "legend_labels = [f'Cluster {cluster+1}' for cluster in ordered_labels]\n",
    "ax.legend(labels=legend_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_clusters': [2, 3, 4, 5],\n",
    "    'affinity': ['nearest_neighbors', 'rbf'],\n",
    "    'n_neighbors': [5, 10, 15],\n",
    "}\n",
    "\n",
    "best_score = -1\n",
    "best_params = {}\n",
    "\n",
    "# Perform grid search\n",
    "for n_clusters in param_grid['n_clusters']:\n",
    "    for affinity in param_grid['affinity']:\n",
    "        for n_neighbors in param_grid['n_neighbors']:\n",
    "            model = SpectralClustering(\n",
    "                n_clusters=n_clusters,\n",
    "                affinity=affinity,\n",
    "                n_neighbors=n_neighbors,\n",
    "                random_state=42\n",
    "            )\n",
    "            labels = model.fit_predict(X)\n",
    "            score = silhouette_score(X, labels)\n",
    "\n",
    "            print(f\"Parameters: n_clusters={n_clusters}, affinity={affinity}, n_neighbors={n_neighbors}, Score: {score}\")\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'n_clusters': n_clusters, 'affinity': affinity, 'n_neighbors': n_neighbors}\n",
    "\n",
    "print(f\"Best parameters: {best_params}, Best silhouette score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (you may need to adjust this)\n",
    "n_clusters = 4\n",
    "\n",
    "# Create a spectral clustering model\n",
    "spectral = SpectralClustering(n_clusters=n_clusters, affinity='rbf', random_state=42)\n",
    "\n",
    "# Fit the model and get cluster assignments\n",
    "labels = spectral.fit_predict(X_pca)\n",
    "\n",
    "plot3dClusterRepresentation(labels,4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "X_standardized = X\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Fine-tune DBSCAN parameters\n",
    "eps_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "min_samples_values = [ 40, 45, 50, 75, 100, 200, 300]\n",
    "\n",
    "best_score = float('inf')  # Initialize with a high value\n",
    "best_params = {}\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_labels = dbscan.fit_predict(X_pca)\n",
    "\n",
    "        # Check if there are at least two clusters for Davies-Bouldin Index\n",
    "        if len(np.unique(dbscan_labels)) >= 2:\n",
    "            silhouette = davies_bouldin_score(X_pca, dbscan_labels)\n",
    "            if silhouette < best_score:\n",
    "                best_score = silhouette\n",
    "                best_params['eps'] = eps\n",
    "                best_params['min_samples'] = min_samples\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Davies-Bouldin Index: {best_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Fine-tune Agglomerative Clustering parameters\n",
    "n_clusters_values = [ 4, 5, 6, 7, 8, 9, 10]\n",
    "linkage_methods = [ 'complete', 'average']\n",
    "affinity_metrics = ['euclidean', 'cosine']\n",
    "\n",
    "best_score = float('inf')  # Initialize with a high value\n",
    "best_params = {}\n",
    "\n",
    "for n_clusters in n_clusters_values:\n",
    "    for linkage in linkage_methods:\n",
    "        for affinity in affinity_metrics:\n",
    "            agglomerative = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage, metric=affinity)\n",
    "            agglomerative_labels = agglomerative.fit_predict(X_pca)\n",
    "\n",
    "            silhouette = davies_bouldin_score(X_pca, agglomerative_labels)\n",
    "            if silhouette < best_score:\n",
    "                best_score = silhouette\n",
    "                best_params['n_clusters'] = n_clusters\n",
    "                best_params['linkage'] = linkage\n",
    "                best_params['metric'] = affinity\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Davies-Bouldin Index: {best_score:.2f}\")\n",
    "\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the linkage matrix\n",
    "# You may need to adjust the method and metric based on your specific case\n",
    "# More information: https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
    "linkage_matrix = linkage(X_pca, method='average', metric='euclidean')\n",
    "\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the dendrogram\n",
    "dendrogram(linkage_matrix)\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_standardized = X\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=1, min_samples=40)\n",
    "dbscan_labels = dbscan.fit_predict(X_pca)\n",
    "\n",
    "# CURE\n",
    "# Note: CURE is not directly available in scikit-learn, and you may need to use external libraries or implement it manually.\n",
    "\n",
    "# Hierarchical Agglomerative Clustering\n",
    "agglomerative = AgglomerativeClustering(n_clusters=10, linkage='average')\n",
    "agglomerative_labels = agglomerative.fit_predict(X_pca)\n",
    "\n",
    "\n",
    "# Evaluate clusters using Davies-Bouldin index\n",
    "agglomerative_davies_bouldin = davies_bouldin_score(X_pca, agglomerative_labels)\n",
    "dbscan_davies_bouldin = davies_bouldin_score(X_pca, dbscan_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pyclustering.cluster.cure import cure\n",
    "from pyclustering.utils import read_sample\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = MinMaxScaler((-2,2)).fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "\n",
    "# Number of clusters you want to obtain\n",
    "num_clusters = 4\n",
    "\n",
    "# CURE clustering\n",
    "cure_instance = cure(data=X_pca,number_represent_points=50, number_cluster=num_clusters, compression=0.8)\n",
    "cure_instance.process()\n",
    "\n",
    "# Get cluster labels\n",
    "cure_clusters = cure_instance.get_clusters()\n",
    "\n",
    "# Visualize clusters (2D visualization)\n",
    "visualizer = cluster_visualizer()\n",
    "visualizer.append_clusters(cure_clusters, X_pca)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Columns to generate data for\n",
    "columns_to_fill = [\n",
    "    'air_quality_(year-round)', 'cashless_society', 'cost_of_living',\n",
    "    'female_friendly', 'freedom_of_speech', 'friendly_to_foreigners', 'fun', 'happiness',\n",
    "    'healthcare', 'peace', 'quality_of_life', 'racial_tolerance', 'religious_government',\n",
    "    'safe_tap_water', 'safety', 'startup_score', 'traffic_safety', 'walkability'\n",
    "]\n",
    "\n",
    "# Manually filled row based on the described preferences\n",
    "user_profile = [{\n",
    "    'air_quality_(year-round)': 5.0,  # Assuming high air quality\n",
    "    'cashless_society': 4.0,  # Assuming moderately cashless\n",
    "    'cost_of_living': 5.0,  # Preferring minimum cost of living\n",
    "    'cost_of_living_for_local': 5.0,  # Preferring minimum cost of living\n",
    "    'female_friendly': 4.0,  # Assuming female-friendly environment\n",
    "    'freedom_of_speech': 5.0,  # Assuming high freedom of speech\n",
    "    'friendly_to_foreigners': 4.0,  # Assuming friendly to foreigners\n",
    "    'fun': 5.0,  # Assuming high fun and nightlife\n",
    "    'happiness': 4.0,  # Assuming generally happy environment\n",
    "    'healthcare': 4.0,  # Assuming good healthcare\n",
    "    'peace': 5.0,  # Assuming peaceful environment\n",
    "    'quality_of_life': 4.0,  # Assuming good quality of life\n",
    "    'racial_tolerance': 4.0,  # Assuming racial tolerance\n",
    "    'religious_government': 3.0,  # Assuming moderate stance on religious government\n",
    "    'safe_tap_water': 5.0,  # Assuming safe tap water\n",
    "    'safety': 4.0,  # Assuming a generally safe environment\n",
    "    'startup_score': 4.0,  # Assuming a moderate startup environment\n",
    "    'traffic_safety': 4.0,  # Assuming moderate traffic safety\n",
    "    'walkability': 3.0 # Assuming moderate walkability\n",
    "}]\n",
    "\n",
    "# Create a DataFrame\n",
    "user_profile_df = pd.DataFrame(user_profile)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Add back the column labels to the rounded features\n",
    "rounded_features.columns = df_for_clustering.columns\n",
    "\n",
    "sub = rounded_features[columns_to_fill]\n",
    "\n",
    "# Calculate cosine similarity matrix for items (rows)\n",
    "item_similarity_matrix = cosine_similarity(sub,user_profile_df)\n",
    "\n",
    "# Find the most similar item\n",
    "most_similar_item_index = np.argmax(item_similarity_matrix)\n",
    "\n",
    "# Display the most similar item and its cluster\n",
    "most_similar_item_cluster = df.loc[most_similar_item_index, 'Cluster']\n",
    "\n",
    "print(f\"The most similar item is in Cluster: {most_similar_item_cluster}\")\n",
    "print(\"Details of the most similar item:\")\n",
    "print(df.iloc[most_similar_item_index])\n",
    "\n",
    "# Filter items within the same cluster\n",
    "cluster_items = df[df['Cluster'] == most_similar_item_cluster]\n",
    "\n",
    "# Display the top 5 items by rating within the same cluster\n",
    "top_5_items = cluster_items.sort_values(by='cost_of_living', ascending=False).head(5)\n",
    "print(f\"Top 5 items within Cluster {most_similar_item_cluster} by rating:\")\n",
    "print(top_5_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually filled row based on the described preferences\n",
    "user_profile = [{\n",
    "    'air_quality_(year-round)': 3.0,  # Assuming high air quality\n",
    "    'cashless_society': 4.0,  # Assuming moderately cashless\n",
    "    'cost_of_living': 5.0,  # Preferring minimum cost of living\n",
    "    'cost_of_living_for_local': 5.0,  # Preferring minimum cost of living\n",
    "    'female_friendly': 4.0,  # Assuming female-friendly environment\n",
    "    'freedom_of_speech': 4.0,  # Assuming high freedom of speech\n",
    "    'friendly_to_foreigners': 4.0,  # Assuming friendly to foreigners\n",
    "    'fun': 5.0,  # Assuming high fun and nightlife\n",
    "    'happiness': 4.0,  # Assuming generally happy environment\n",
    "    'healthcare': 4.0,  # Assuming good healthcare\n",
    "    'internet': 5.0,\n",
    "    'lgbt_friendly': 2.0,\n",
    "    'nightlife': 2.0,\n",
    "    'peace': 5.0,  # Assuming peaceful environment\n",
    "    'quality_of_life': 4.0,  # Assuming good quality of life\n",
    "    'racial_tolerance': 4.0,  # Assuming racial tolerance\n",
    "    'religious_government': 5.0,  # Assuming moderate stance on religious government\n",
    "    'safe_tap_water': 5.0,  # Assuming safe tap water\n",
    "    'safety': 4.0,  # Assuming a generally safe environment\n",
    "    'startup_score': 4.0,  # Assuming a moderate startup environment\n",
    "    'traffic_safety': 4.0,  # Assuming moderate traffic safety\n",
    "    'walkability': 3.0, # Assuming moderate walkability\n",
    "    'nomad_score': 2.0\n",
    "}]\n",
    "\n",
    "# Create a DataFrame\n",
    "user_profile_df = pd.DataFrame(user_profile)\n",
    "\n",
    "# Apply PCA with 3 components\n",
    "X_pca = pca.transform(user_profile_df.values)\n",
    "\n",
    "# Now, let's say you have a new user with preferences\n",
    "new_user_preferences = np.random.rand(1, 3)\n",
    "\n",
    "# Assign the user to the nearest cluster\n",
    "user_cluster = kmeans.predict(X_pca)\n",
    "\n",
    "# Print the result\n",
    "print(\"User belongs to Cluster:\", user_cluster[0])\n",
    "\n",
    "# Filter items within the same cluster\n",
    "cluster_items = df[df['Cluster'] == user_cluster[0]]\n",
    "\n",
    "\n",
    "# Display the top 5 items by rating within the same cluster\n",
    "top_5_items = cluster_items.sort_values(by='nomad_score', ascending=False)\n",
    "print(df[df['Cluster'] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chwEFCON9UBq",
    "outputId": "eecb5c38-8ac1-48b6-e90f-18769eaadf97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1br_studio_rent_in_center    0\n",
       "adult_nightlife              0\n",
       "air_quality_(year-round)     0\n",
       "airbnb_(monthly)             0\n",
       "cashless_society             0\n",
       "coca-cola                    0\n",
       "coffee                       0\n",
       "cost_of_living               0\n",
       "cost_of_living_for_expat     0\n",
       "cost_of_living_for_local     0\n",
       "female_friendly              0\n",
       "freedom_of_speech            0\n",
       "friendly_to_foreigners       0\n",
       "fun                          0\n",
       "happiness                    0\n",
       "healthcare                   0\n",
       "internet                     0\n",
       "lgbt_friendly                0\n",
       "nightlife                    0\n",
       "peace                        0\n",
       "quality_of_life              0\n",
       "racial_tolerance             0\n",
       "religious_government         0\n",
       "safe_tap_water               0\n",
       "safety                       0\n",
       "startup_score                0\n",
       "traffic_safety               0\n",
       "walkability                  0\n",
       "nomad_score                  0\n",
       "region                       0\n",
       "country                      0\n",
       "place_slug                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to normalize the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_nomadlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = ['country', 'place_slug', 'region']\n",
    "\n",
    "columns_to_normalize = [col for col in df.columns if col not in columns_to_exclude]\n",
    "\n",
    "# Crea un MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normaliza las columnas seleccionadas\n",
    "df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fun'] = df['fun'].round().astype(int)\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing\n",
    "Minhashing is a technique in data analysis that quickly estimates the similarity between two sets without comparing every element. It works by using multiple hash functions to create compact signatures for sets. These signatures capture essential information about the sets' contents, allowing for efficient comparison. The similarity between sets is approximated by comparing the fraction of matching hash codes. Minhashing is particularly useful for tasks like duplicate detection and recommendation systems, providing a fast and scalable way to gauge set similarity in large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "def measure_memory():\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024 / 1024  # Convertir a megabytes\n",
    "\n",
    "# Columns for Minhashing\n",
    "minhash_columns = ['1br_studio_rent_in_center', 'adult_nightlife', 'air_quality_(year-round)', 'airbnb_(monthly)',\n",
    "                    'cashless_society', 'coca-cola', 'coffee', 'cost_of_living', 'cost_of_living_for_expat',\n",
    "                    'cost_of_living_for_local', 'female_friendly', 'freedom_of_speech', 'friendly_to_foreigners',\n",
    "                    'fun', 'happiness', 'healthcare', 'internet', 'lgbt_friendly', 'nightlife', 'peace',\n",
    "                    'quality_of_life', 'racial_tolerance', 'religious_government', 'safe_tap_water', 'safety',\n",
    "                    'startup_score', 'traffic_safety', 'walkability', 'nomad_score',\n",
    "                    'Access to electricity (% of population)', 'Access to electricity, urban (% of urban population)',\n",
    "                    'Renewable electricity output (% of total electricity output)']\n",
    "\n",
    "# Convert DataFrame values to a list of sets\n",
    "sets = df[minhash_columns].values.tolist()\n",
    "\n",
    "# Number of permutation functions for Minhashing\n",
    "num_perm = 128\n",
    "\n",
    "# Create MinHash objects for each set\n",
    "minhashes = []\n",
    "total_memory = 0\n",
    "start_memory = measure_memory()\n",
    "start_time = time.time() * 1000  # Start time in milliseconds\n",
    "for s in sets:\n",
    "    minhash = MinHash(num_perm=num_perm)\n",
    "    for value in s:\n",
    "        minhash.update(str(value).encode('utf-8'))\n",
    "    minhashes.append(minhash)\n",
    "end_time = time.time() * 1000  # End time in milliseconds\n",
    "end_memory = measure_memory()\n",
    "elapsed_time = end_time - start_time\n",
    "memory_usage = end_memory - start_memory\n",
    "total_memory += memory_usage\n",
    "print(f\"Time to create MinHash objects: {elapsed_time} milliseconds\")\n",
    "print(f\"Memory usage: {memory_usage} MB\")\n",
    "print()\n",
    "\n",
    "# Create LSH index (Locality-Sensitive Hashing)\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=num_perm)\n",
    "start_memory = measure_memory()\n",
    "start_time = time.time() * 1000  # Start time in milliseconds\n",
    "for i, minhash in enumerate(minhashes):\n",
    "    lsh.insert(i, minhash)\n",
    "end_time = time.time() * 1000  # End time in milliseconds\n",
    "end_memory = measure_memory()\n",
    "elapsed_time = end_time - start_time\n",
    "memory_usage = end_memory - start_memory\n",
    "print(f\"Time to create LSH index: {elapsed_time} milliseconds\")\n",
    "print(f\"Memory usage: {memory_usage} MB\")\n",
    "total_memory += memory_usage\n",
    "print()\n",
    "\n",
    "# Perform an LSH query to find similar sets\n",
    "query_number = 78\n",
    "query_set = df.loc[query_number, minhash_columns].tolist()\n",
    "minhash_query = MinHash(num_perm=num_perm)\n",
    "for value in query_set:\n",
    "    minhash_query.update(str(value).encode('utf-8'))\n",
    "\n",
    "# Find similar sets using LSH\n",
    "start_memory = measure_memory()\n",
    "start_time = time.time() * 1000  # Start time in milliseconds\n",
    "similar_sets = lsh.query(minhash_query)\n",
    "end_time = time.time() * 1000  # End time in milliseconds\n",
    "end_memory = measure_memory()\n",
    "elapsed_time = end_time - start_time\n",
    "memory_usage = end_memory - start_memory\n",
    "print(f\"Total memory used: {total_memory} MB\\n\")\n",
    "print(f\"Sets similar to {query_number}: {similar_sets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to show the similarity between the items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes\n",
    "fila_consulta_idx = 83\n",
    "fila_predicha_idx = 78\n",
    "\n",
    "# Value of the rows\n",
    "fila_consulta = df.loc[fila_consulta_idx, minhash_columns].values\n",
    "fila_predicha = df.loc[fila_predicha_idx, minhash_columns].values\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(minhash_columns, fila_consulta, marker='o', label=f'Fila {fila_consulta_idx}', color='blue')\n",
    "plt.plot(minhash_columns, fila_predicha, marker='x', label=f'Fila {fila_predicha_idx}', color='green')\n",
    "\n",
    "plt.title(f\"comparison between row {fila_consulta_idx} and row {fila_predicha_idx}\")\n",
    "plt.xlabel(\"Columnas\")\n",
    "plt.ylabel(\"Valores\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_row_idx = 83\n",
    "predicted_row_idx = 78\n",
    "\n",
    "# Get values of the rows\n",
    "query_row_values = df.loc[query_row_idx, minhash_columns].values\n",
    "predicted_row_values = df.loc[predicted_row_idx, minhash_columns].values\n",
    "\n",
    "# Set up stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "colors = ['c', 'm']\n",
    "\n",
    "# Stacked bars for each row\n",
    "ax.bar(minhash_columns, query_row_values, color=colors[0], label=f'Query (Index {query_row_idx})')\n",
    "ax.bar(minhash_columns, predicted_row_values, bottom=query_row_values, color=colors[1], label=f'Prediction (Index {predicted_row_idx})')\n",
    "\n",
    "# Label and legend settings\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Comparison of Features between Query and Prediction')\n",
    "ax.legend()\n",
    "\n",
    "# Rotate x-axis labels for clarity\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.savefig('comparison_plot.pdf', bbox_inches='tight', dpi=300)  # Adjust the file name and DPI as needed\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the stacked bars show the values of the characteristics for each sample. Similarity can be inferred by observing similar patterns and trends in the distribution of values between the two samples. If the bars for the corresponding features are visually similar in height and location, it indicates that the two samples share similar characteristics, which could suggest a similarity in the profile or behaviour represented by those specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row indices to compare\n",
    "query_row_idx = 83\n",
    "predicted_row_idx = 78\n",
    "\n",
    "# Get values from rows\n",
    "query_row_values = df.loc[query_row_idx, minhash_columns].values\n",
    "predicted_row_values = df.loc[predicted_row_idx, minhash_columns].values\n",
    "\n",
    "# Scatter plot configuration\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(query_row_values, predicted_row_values, alpha=0.5, color='blue')\n",
    "plt.plot([min(query_row_values), max(query_row_values)], [min(query_row_values), max(query_row_values)], linestyle='--', color='red')  # Identity line\n",
    "\n",
    "# Label configuration\n",
    "plt.title(f\"Comparison between Query (Index {query_row_idx}) and Prediction (Index {predicted_row_idx})\")\n",
    "plt.xlabel(\"Query\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Blue Points**: Each blue point on the graph represents a pair of values from the query and prediction rows at corresponding positions. Each x and y coordinate of the point represents the respective value in the query and prediction rows.\n",
    "\n",
    "**Dotted Red Identity Line**: The red dotted line represents the identity line, where x and y values are equal. In a scatter plot, this should be a diagonal line from the bottom-left corner to the top-right corner. It serves as a reference to compare values from the query and prediction rows. If all blue points were on this line, it would mean that the values perfectly match.\n",
    "\n",
    "The presence of blue points near the identity line suggests similarity between the values of the query and prediction rows at those positions. The dispersion around the identity line indicates the magnitude of differences between the values of the query and prediction. The absence of blue points directly on the identity line could be due to values not being exactly equal at those specific positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of the rows you want to compare\n",
    "query_row_idx = 83\n",
    "predicted_row_idx = 78\n",
    "\n",
    "# Get values of the rows\n",
    "query_row = df.loc[query_row_idx, minhash_columns].values\n",
    "predicted_row = df.loc[predicted_row_idx, minhash_columns].values\n",
    "\n",
    "# Column names for the radar plot\n",
    "column_names = minhash_columns\n",
    "\n",
    "# Number of columns\n",
    "num_columns = len(column_names)\n",
    "\n",
    "# Angle of each axis in the radar plot\n",
    "angles = np.linspace(0, 2 * np.pi, num_columns, endpoint=False).tolist()\n",
    "\n",
    "# Close the radar plot\n",
    "query_row = np.concatenate((query_row, [query_row[0]]))\n",
    "predicted_row = np.concatenate((predicted_row, [predicted_row[0]]))\n",
    "angles += angles[:1]\n",
    "\n",
    "# Configure the plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Change colors\n",
    "ax.fill(angles, query_row, 'c', alpha=0.1, label='Query')  # Cyan\n",
    "ax.fill(angles, predicted_row, 'm', alpha=0.1, label='Prediction')  # Magenta\n",
    "\n",
    "# Configure labels and legend\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(column_names)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Add a title\n",
    "plt.title(f\"Comparison between Query (Index {query_row_idx}) and Prediction (Index {predicted_row_idx})\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity\n",
    "Cosine similarity is a measure commonly used to assess the similarity between two vectors, often applied in the context of similar items. It quantifies the cosine of the angle between these vectors, providing a value between -1 and 1. A value close to 1 implies high similarity, indicating shared characteristics. This metric is particularly useful for item recommendation systems, as it assesses similarity based on the direction of features, regardless of vector magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "selected_columns = ['cost_of_living', 'fun', 'safety']\n",
    "features = df[selected_columns]\n",
    "\n",
    "# Get user preferences within the range of 1-5\n",
    "user_preferences = {}\n",
    "for col in selected_columns:\n",
    "    while True:\n",
    "        user_input = input(f\"Enter a value for {col} (1-5): \")\n",
    "        try:\n",
    "            user_value = float(user_input)\n",
    "            if 1 <= user_value <= 5:\n",
    "                user_preferences[col] = user_value\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please enter a value between 1 and 5.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "\n",
    "# Create a DataFrame with user preferences\n",
    "user_row = pd.DataFrame([user_preferences])\n",
    "\n",
    "# Add the user to the features\n",
    "features = pd.concat([features, user_row], ignore_index=True)\n",
    "print()\n",
    "\n",
    "# Impute NaN values with the mean of each column\n",
    "start_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "start_time = time.time() * 1000  # Start time in milliseconds\n",
    "features = features.apply(lambda col: col.fillna(col.mean()), axis=0)\n",
    "end_time = time.time() * 1000  # End time in milliseconds\n",
    "end_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "memory_usage = end_memory - start_memory\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time to impute NaN values: {elapsed_time} milliseconds\\n\")\n",
    "\n",
    "# Scale the features to ensure they have the same weight\n",
    "start_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "start_time = time.time() * 1000  # Start time in milliseconds\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "end_time = time.time() * 1000  # End time in milliseconds\n",
    "end_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "memory_usage = end_memory - start_memory\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time to scale features: {elapsed_time} milliseconds\")\n",
    "print(f\"Memory usage: {np.abs(memory_usage)} MB\\n\")\n",
    "\n",
    "# Calculate cosine similarity between the elements\n",
    "start_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "start_time = time.time() * 1000  # Start time in milliseconds\n",
    "cosine_sim = cosine_similarity(features_scaled, features_scaled)\n",
    "end_time = time.time() * 1000  # End time in milliseconds\n",
    "end_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "memory_usage = end_memory - start_memory\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time to calculate cosine similarity: {elapsed_time} milliseconds\")\n",
    "print(f\"Memory usage: {np.abs(memory_usage)} MB\\n\")\n",
    "\n",
    "# Get similarities between recommendations and other elements\n",
    "user_index = len(features) - 1  # Last row is the user\n",
    "start_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "start_time = time.time() * 1000  # Start time in milliseconds\n",
    "user_similarities = cosine_sim[user_index, :]\n",
    "end_time = time.time() * 1000  # End time in milliseconds\n",
    "end_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "memory_usage = end_memory - start_memory\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time to get similarities: {elapsed_time} milliseconds\")\n",
    "print(f\"Memory usage: {np.abs(memory_usage)} MB\\n\")\n",
    "\n",
    "# Sort the elements based on similarity\n",
    "start_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "start_time = time.time() * 1000  # Start time in milliseconds\n",
    "similar_elements = user_similarities.argsort()[::-1]\n",
    "end_time = time.time() * 1000  # End time in milliseconds\n",
    "end_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "memory_usage = end_memory - start_memory\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Show the recommendations (e.g., the top 3 most similar elements)\n",
    "top_recommendations = similar_elements[1:4]\n",
    "print()\n",
    "print(\"Top 3 recommendations:\", top_recommendations)\n",
    "\n",
    "# Find the most common country among the top 3 recommendations\n",
    "start_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "start_time = time.time() * 1000  # Start time in milliseconds\n",
    "most_common_country = df.loc[top_recommendations, 'country'].mode()[0]\n",
    "end_time = time.time() * 1000  # End time in milliseconds\n",
    "end_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "memory_usage = end_memory - start_memory\n",
    "elapsed_time = end_time - start_time\n",
    "print()\n",
    "print(f\"Time to find the most common country: {elapsed_time} milliseconds\")\n",
    "print(f\"Total memory usage: {end_memory} MB\\n\")\n",
    "print(\"Recommended country based on similarity:\", most_common_country)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard similarity\n",
    "Now we want to try Jaccard, first we are going to convert columns (1-5 rating), to \n",
    "very bad, bad, normal, good, very good.\n",
    "\n",
    "For example, a value of 1 in the cost variable means that the country is very expensive. A value of 1 for fun means that the country is not fun. And a value of 1 for safety means that the country is not safe.\n",
    "\n",
    "\n",
    "Jaccard similarity works well with categorical data because it focuses on shared elements, making it effective for variables where values represent distinct categories without inherent numerical relationships. It is robust to variations in set size and ignores the order or frequency of elements, making it suitable for scenarios where the presence or absence of specific categories matters more than their numerical values. In essence, Jaccard similarity aligns with the natural understanding of similarity in categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_nomadlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rows = df.sample(5, random_state=42)[['cost_of_living','fun', 'safety', 'country']]\n",
    "\n",
    "# Imprimir las filas seleccionadas\n",
    "print(\"Selected rows for different values:\")\n",
    "print(sample_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will categorise the variables in order to calculate the Jaccard Similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fun'] = df['fun'].round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fun'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_labels = {1: 'Not Fun', 2: 'Somewhat Fun', 3: 'Moderately Fun', 4: 'Very Fun'}\n",
    "\n",
    "df['fun'] = df['fun'].map(fun_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fun'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['safety'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom labels for 'safety'\n",
    "safety_labels = {\n",
    "    1.0: 'Very Unsafe',\n",
    "    2.0: 'Unsafe',\n",
    "    3.0: 'Moderate Safety',\n",
    "    4.0: 'Safe'\n",
    "}\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df['safety'] = df['safety'].map(safety_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['safety'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cost_of_living'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom labels for 'cost_of_living'\n",
    "cost_labels = {\n",
    "    1.0: 'Very Expensive',\n",
    "    2.0: 'Expensive',\n",
    "    3.0: 'Moderate Cost',\n",
    "    4.0: 'Very Cheap'\n",
    "}\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df['cost_of_living'] = df['cost_of_living'].map(cost_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cost_of_living'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select non-numeric columns\n",
    "non_numeric_columns = df.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# Display the non-numeric columns\n",
    "print(\"Non-numeric columns:\", non_numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to exclude from standardization\n",
    "columns_to_exclude = ['cost_of_living', 'safety', 'region', 'country', 'place_slug', 'fun_category']\n",
    "\n",
    "# Select numerical columns for standardization\n",
    "numerical_columns = [col for col in df.columns if col not in columns_to_exclude and df[col].dtype != 'object']\n",
    "\n",
    "df_standardized = df.copy()\n",
    "\n",
    "# Standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "df_standardized[numerical_columns] = scaler.fit_transform(df_standardized[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standardized.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standardized.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is categorized and standarized.\n",
    "\n",
    "We are going to perform jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns for Jaccard similarity\n",
    "jaccard_columns = ['1br_studio_rent_in_center', 'adult_nightlife', 'air_quality_(year-round)', 'airbnb_(monthly)',\n",
    "                    'cashless_society', 'coca-cola', 'coffee', 'cost_of_living', 'cost_of_living_for_expat',\n",
    "                    'cost_of_living_for_local', 'female_friendly', 'freedom_of_speech', 'friendly_to_foreigners',\n",
    "                    'fun', 'happiness', 'healthcare', 'internet', 'lgbt_friendly', 'nightlife', 'peace',\n",
    "                    'quality_of_life', 'racial_tolerance', 'religious_government', 'safe_tap_water', 'safety',\n",
    "                    'startup_score', 'traffic_safety', 'walkability', 'nomad_score',\n",
    "                    'Access to electricity (% of population)', 'Access to electricity, urban (% of urban population)',\n",
    "                    'Renewable electricity output (% of total electricity output)']\n",
    "\n",
    "# Filter non-numeric columns\n",
    "numeric_columns = df_standardized[jaccard_columns].select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Fill NaN values with 0\n",
    "df_standardized[numeric_columns] = df_standardized[numeric_columns].fillna(0)\n",
    "\n",
    "# Selecting the rows for Jaccard similarity\n",
    "query_row = df_standardized.loc[78, numeric_columns].astype(float)\n",
    "other_rows = df_standardized.loc[:, numeric_columns].astype(float)\n",
    "\n",
    "# Measure memory before calculation\n",
    "start_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "\n",
    "# Measure time before calculation\n",
    "start_time = time.time() * 1000  # Start time in milliseconds\n",
    "\n",
    "# Calculating Jaccard similarities\n",
    "intersection = other_rows.apply(lambda row: np.sum(np.logical_and(query_row, row)), axis=1)\n",
    "union = other_rows.apply(lambda row: np.sum(np.logical_or(query_row, row)), axis=1)\n",
    "jaccard_similarities = intersection / union\n",
    "\n",
    "# Measure time after calculation\n",
    "end_time = time.time() * 1000  # End time in milliseconds\n",
    "\n",
    "# Measure memory after calculation\n",
    "end_memory = psutil.virtual_memory().used / 1024 / 1024\n",
    "\n",
    "# Getting indices of top 3 similar rows\n",
    "top_indices = np.argsort(jaccard_similarities)[-3:][::-1]\n",
    "\n",
    "# Eliminate the query row from the top recommendations\n",
    "# Get the indices of the 3 most similar recommendations\n",
    "top_indices_without_query = top_indices[top_indices != 78][:3]\n",
    "\n",
    "print(\"Recommended country:\", df_standardized.loc[top_indices_without_query, 'country'].mode()[0])\n",
    "print()\n",
    "# Display time and memory information\n",
    "elapsed_time = end_time - start_time\n",
    "memory_usage = end_memory - start_memory\n",
    "print(f\"Time taken to calculate Jaccard similarity: {elapsed_time} milliseconds\\n\")\n",
    "print(f\"Memory usage: {np.abs(memory_usage)} MB\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User recommendations\n",
    "In the following cells, we are going to train a matrix factorization algorithm. In particolar, the algorithm is UV matrix factorization. The algorithm is optimized so to make it run faster. In particular, Stochastic gradient descent is used instead of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering with matrix factorization\n",
    "Here, we compute 2 matrices that multiplied together approximate the initial rating matrix. These two matrices are trained iteratively, using Mean Squared Error to get closer to the real matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jPsMPdc9T_Z",
    "outputId": "1982bf63-6c62-4551-9fc4-fcfae713f788"
   },
   "outputs": [],
   "source": [
    "def matrix_factorization(matrix, size=15, steps=500, eta = 0.01, lambd = 0.005, threshold = 0.01):\n",
    "    user_len = len(matrix[:, 0])\n",
    "    item_len = len(matrix[0,:])\n",
    "\n",
    "    decomp_rows = np.random.rand(user_len,size)\n",
    "    decomp_cols = np.random.rand(item_len, size)\n",
    "    \n",
    "     \n",
    "    decomp_cols = decomp_cols.T\n",
    "\n",
    "    for step in tqdm(range(steps)):\n",
    "        for i in range(len(matrix)):\n",
    "            for j in range(len(matrix[i])):\n",
    "                if matrix[i][j] != 0:\n",
    "                    #Compute gradient of MSE\n",
    "                    err_ij = matrix[i][j] -np.dot(decomp_rows[i,:], decomp_cols[:,j])\n",
    "\n",
    "                    for k in range(size):\n",
    "                        #Update with gradient descent and regularization factor\n",
    "                        decomp_rows[i][k] = decomp_rows[i][k] + eta * (2*err_ij*decomp_cols[k][j] - lambd * decomp_rows[i][k])\n",
    "                        decomp_cols[k][j] = decomp_cols[k][j] + eta * (2*err_ij*decomp_rows[i][k] - lambd * decomp_cols[k][j])\n",
    "\n",
    "        err = np.dot(decomp_rows, decomp_cols)\n",
    "        e = 0\n",
    "\n",
    "        #Testing\n",
    "        for i in range(len(matrix)):\n",
    "            for j in range(len(matrix[i])):\n",
    "                if matrix[i][j]!=0:\n",
    "                    e = e + pow(matrix[i][j] - np.dot(decomp_rows[i,:], decomp_cols[:,j]) ,2)\n",
    "\n",
    "        if e < threshold:\n",
    "            break\n",
    "    \n",
    "    return decomp_rows, decomp_cols.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "gh2Ao7imI83r",
    "outputId": "a8d9f080-cc0c-4fc2-dc9f-b6ca146e9d82"
   },
   "outputs": [],
   "source": [
    "#Create the matrices to factorize\n",
    "matrix_to_factorize_0_1 = np.zeros((len(users), len(cities_list)), dtype='bool')\n",
    "matrix_to_factorize_rated = np.zeros((len(users), len(cities_list)), dtype='int')\n",
    "\n",
    "#Populate them\n",
    "for i,city in enumerate(cities_list):\n",
    "    for j,(user,city_values) in enumerate(users.items()):\n",
    "        if city in city_values:\n",
    "            matrix_to_factorize_0_1[j][i] = 1\n",
    "            matrix_to_factorize_rated[j][i] = users_cities_rating[user][city]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [05:42<00:00, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 51s, sys: 13.4 s, total: 6min 5s\n",
      "Wall time: 5min 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time decomp_users_0_1, decomp_cities_0_1 = matrix_factorization(matrix_to_factorize_0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [01:58<00:00,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 7s, sys: 12.3 s, total: 2min 20s\n",
      "Wall time: 1min 58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time decomp_users_rated, decomp_cities_rated = matrix_factorization(matrix_to_factorize_rated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(decomp_rows, decomp_cols, users, cities_list, first_n=10):\n",
    "    best_cities = []\n",
    "    for index, (user, cities) in enumerate(users.items()):\n",
    "        scores = np.dot(decomp_rows[index,:], decomp_cols[:,:])\n",
    "        scores_aug = [(m,n) for m,n in zip(cities_list , scores)]\n",
    "        scores_aug_sorted = sorted(scores_aug, key = lambda x:x[1], reverse = True)\n",
    "        score_sorted_cities = [city for city, score in scores_aug_sorted] \n",
    "        best_cities.append(set(score_sorted_cities[:first_n]))\n",
    "    return best_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_0_1 = testing(decomp_users_0_1, decomp_cities_0_1.T, users, cities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_rated = testing(decomp_users_rated, decomp_cities_rated.T, users, cities_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing the same with a SoTA user based recommendation framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_1 = LightFM(no_components=30)\n",
    "model_rated = LightFM(no_components=30)\n",
    "\n",
    "matrix_to_factorize_neg_pos = np.zeros((len(users), len(cities_list)), dtype='short')\n",
    "matrix_to_factorize_rated = np.zeros((len(users), len(cities_list)), dtype='short')\n",
    "\n",
    "\n",
    "#Populate them\n",
    "for i,city in enumerate(cities_list):\n",
    "    for j,(user,city_values) in enumerate(users.items()):\n",
    "        if city in city_values:\n",
    "            matrix_to_factorize_0_1[j][i] = 1\n",
    "            matrix_to_factorize_rated[j][i] = users_cities_rating[user][city]\n",
    "            '''\n",
    "            if users_cities_rating[user][city] > 3:\n",
    "                matrix_to_factorize_rated[j][i] = 1\n",
    "            else:\n",
    "                matrix_to_factorize_rated[j][i] = -1\n",
    "            '''\n",
    "        \n",
    "\n",
    "\n",
    "matrix_to_factorize_0_1_coo = sparse.coo_matrix(matrix_to_factorize_0_1)\n",
    "matrix_to_factorize_rated_coo = sparse.coo_matrix(matrix_to_factorize_rated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.3 s, sys: 0 ns, total: 13.3 s\n",
      "Wall time: 13.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7ff74f7c0910>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model_0_1.fit(matrix_to_factorize_0_1_coo, epochs=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.77 s, sys: 0 ns, total: 2.77 s\n",
      "Wall time: 2.77 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7ff74f7c0730>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model_rated.fit(matrix_to_factorize_rated_coo, epochs=600);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_SoTA(model, users, cities_list, first_n=10):\n",
    "    best_cities = []\n",
    "    for index, (user, cities) in enumerate(users.items()):\n",
    "        #print(f\"User has been to {list(users.items())[user_id]}\")\n",
    "        scores = model.predict(index, np.arange(len(cities_list)))\n",
    "        scores_aug = [(m,n) for m,n in zip(cities_list , scores)]\n",
    "        scores_aug_sorted = sorted(scores_aug, key = lambda x:x[1], reverse = True)\n",
    "        score_sorted_cities = [city for city, score in scores_aug_sorted] \n",
    "        best_cities.append(set(score_sorted_cities[:first_n]))\n",
    "    return best_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_sota_0_1 = testing_SoTA(model_0_1, users, cities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cities_sota_rated = testing_SoTA(model_rated, users, cities_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping(recommendations, recommendations_sota, first_n=10):\n",
    "    total_overlap = []\n",
    "    for i in range(len(recommendations)):\n",
    "        overlap = recommendations[i] & recommendations_sota[i]\n",
    "        total_overlap.append(len(overlap)/first_n)\n",
    "    return sum(total_overlap)/len(total_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlapping of the boolean model is 0.04775819626272875\n",
      "Overlapping of the rated model is 0.28915463802171293\n"
     ]
    }
   ],
   "source": [
    "boolean_overlap = overlapping(best_cities_0_1, best_cities_sota_0_1)\n",
    "print(f\"Overlapping of the boolean model is {boolean_overlap}\")\n",
    "\n",
    "rated_overlap = overlapping(best_cities_rated, best_cities_sota_rated)\n",
    "print(f\"Overlapping of the rated model is {rated_overlap}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
